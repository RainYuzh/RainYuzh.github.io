{"meta":{"title":"RainYu","subtitle":"","description":"","author":"RainYu","url":"http://example.com","root":"/"},"pages":[{"title":"about","date":"2023-12-21T02:21:44.000Z","updated":"2023-12-21T02:21:44.628Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":""},{"title":"tags","date":"2023-12-21T02:22:05.000Z","updated":"2023-12-21T02:23:08.584Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2023-12-21T02:21:55.000Z","updated":"2023-12-21T02:22:44.584Z","comments":false,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"[文献阅读]A Watermark for Large Language Models","slug":"文献阅读-A-Watermark-for-Large-Language-Models","date":"2024-01-03T07:53:07.000Z","updated":"2024-01-03T07:54:48.923Z","comments":true,"path":"2024/01/03/文献阅读-A-Watermark-for-Large-Language-Models/","permalink":"http://example.com/2024/01/03/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB-A-Watermark-for-Large-Language-Models/","excerpt":"时间：2023.1 作者：John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, Tom Goldstein 会议：ICML’23 开源：https://github.com/jwkirchenbauer/lm-watermarking 大型语言模型可能产生的潜在危害可以通过在生成的文本中嵌入水印来降低,即在文本片段中的一小段连续词汇中嵌入难以被人眼察觉但可以通过算法探测的信号。我们提出了一种水印框架,用于专有的语言模型。水印可以嵌入文本中而几乎不影响文本质量,并且可以使用开源算法进行检测,而不需要访问语言模型的API或参数。此水印通过在生成词汇前选择一组随机的”绿色”词汇,并在取样过程中轻微地提高这些”绿色”词汇的使用概率来实现。我们提出了一种统计检验方法来检测水印,该方法可以给出容易解释的P值,并给出了分析水印敏感性的信息论框架。我们在OPT家族的多亿参数模型上测试了该水印,并讨论了其鲁棒性和安全性。","text":"时间：2023.1 作者：John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, Tom Goldstein 会议：ICML’23 开源：https://github.com/jwkirchenbauer/lm-watermarking 大型语言模型可能产生的潜在危害可以通过在生成的文本中嵌入水印来降低,即在文本片段中的一小段连续词汇中嵌入难以被人眼察觉但可以通过算法探测的信号。我们提出了一种水印框架,用于专有的语言模型。水印可以嵌入文本中而几乎不影响文本质量,并且可以使用开源算法进行检测,而不需要访问语言模型的API或参数。此水印通过在生成词汇前选择一组随机的”绿色”词汇,并在取样过程中轻微地提高这些”绿色”词汇的使用概率来实现。我们提出了一种统计检验方法来检测水印,该方法可以给出容易解释的P值,并给出了分析水印敏感性的信息论框架。我们在OPT家族的多亿参数模型上测试了该水印,并讨论了其鲁棒性和安全性。 基础知识语言生成模型的基本原理是: 输入一个样本词序列(可以是空的),比如”春风拂人面”。 模型内部含有一个巨大的词汇表,词汇表包含了所有可能出现的词语。 模型根据输入词序列,计算每个词汇表内词语出现的概率,形成一个概率分布。通常使用栈式长短期记忆网络(LSTM)来提取词序列上下文信息,然后计算每个词的条件概率。 根据这个概率分布随机采样获得下一个词。比如”吹”的概率最大,就随机选择”吹”作为下一个词。 把这个新词加入输入序列,执行3-4步,不断扩展序列。 通过反复扩展序列,可以自动生成与输入样本风格相似的新句子,像人类一样进行语言产出。 随着模型规模越来越大,训练数据量也越来越大,生成效果不断提高,产生出来的句子质量和人类语言越来越相似。 但模型内部机制是统计规律,理解能力有限,可能会出现不合逻辑或不正确的情景。所以语言生成只能助于创作,但不等同于真正的语言理解。 以上就是语言生成模型的基本思路和工作流程。它通过统计连续词汇的条件概率来模拟人类语言产出的过程。 文本熵是用来衡量文本随机性程度的一个量度 Figure 1给出了带有水印和无水印的语言模型生成文本示例。 无水印文本采用了整个提示段落(用蓝色标注)作为输入。 带水印文本使用相同提示,但应用了水印技术。 水印技术会给部分词汇赋予“绿色”标签(如图中的绿色词汇)。 检测结果显示,带水印文本生成的“绿色”词汇远超预期值(28个比预期9个多很多)。 以此计算得出的p值非常小(约6×10-14),证明这绝非随机产生,而很可能来自受水印影响的模型。 说明通过统计检测可以很高概率判断带水印文本是否来源于该语言模型。 同时给出了水印参数γ和δ,以供后续实验参考。 利用了OPT-6.7B模型和多项式抽样进行实验。 红单水印机制思路: 考虑到单词水印可能被分词工具破坏,采用短语概率进行水印编码 事先选择一组短语作为水印载体,将其概率提高来生成水印文本 主要流程: 从语料库中采样获取一组长度为k的短语序列作为水印载体 输入语料和诱导前缀,使用 n-gram LM模型计算每个可能短语的概率分布 将水印载体短语的概率提高一个比例因子(1+γ) 根据修改后的概率分布采样选择下一个短语加入水印文本 重复上述步骤生成完整水印文本 检测过程: 统计检测文本中的水印载体短语出现次数Nc 根据 texts长度L和装载体短语总数C,计算期望值E(Nc) 用超额比对Nc和E(Nc),计算p值判断是否含水印 算法二的改进在于利用短语概率而非单词概率进行水印编码,使水印更难被破坏,同时检测方法与算法一相似。","categories":[],"tags":[]},{"title":"LLM综述","slug":"LLM综述","date":"2023-12-22T07:06:14.000Z","updated":"2023-12-22T07:19:25.986Z","comments":true,"path":"2023/12/22/LLM综述/","permalink":"http://example.com/2023/12/22/LLM%E7%BB%BC%E8%BF%B0/","excerpt":"语言本质上是一个由人类表达方式组成的复杂、错综复杂的系统，受语法规则的制约。开发能够理解和掌握语言的人工智能算法是一项重大挑战。作为一种主要方法，语言建模在过去二十年中被广泛用于语言理解和生成，从统计语言模型发展到神经语言模型。最近，预训练语言模型（PLMs）通过在大规模语料库中预训练 Transformer 模型而被提出，在解决各种 NLP 任务方面显示出强大的能力。由于研究人员发现模型缩放可以提高性能，他们进一步研究了缩放效应，将模型规模扩大到更大。有趣的是，当参数比例超过一定水平时，这些扩大的语言模型不仅能显著提高性能，而且还能显示出一些小规模语言模型所不具备的特殊能力。为了区分参数规模的差异，研究界创造了大型语言模型（LLM）这一术语，用于指规模相当大的 PLM。最近，学术界和工业界都在大力推进 LLM 的研究，其中一个显著的进展就是 ChatGPT 的推出，它引起了社会的广泛关注。LLM 的技术演进对整个人工智能界产生了重要影响，它将彻底改变我们开发和使用人工智能算法的方式。在本研究中，我们通过介绍 LLMs 的背景、主要发现和主流技术，回顾了 LLMs 的最新进展。其中，我们重点讨论了 LLMs 的四个主要方面，即预训练、适应性调整、利用和能力评估。此外，我们还总结了开发 LLMs 的可用资源，并讨论了未来发展方向的遗留问题。","text":"语言本质上是一个由人类表达方式组成的复杂、错综复杂的系统，受语法规则的制约。开发能够理解和掌握语言的人工智能算法是一项重大挑战。作为一种主要方法，语言建模在过去二十年中被广泛用于语言理解和生成，从统计语言模型发展到神经语言模型。最近，预训练语言模型（PLMs）通过在大规模语料库中预训练 Transformer 模型而被提出，在解决各种 NLP 任务方面显示出强大的能力。由于研究人员发现模型缩放可以提高性能，他们进一步研究了缩放效应，将模型规模扩大到更大。有趣的是，当参数比例超过一定水平时，这些扩大的语言模型不仅能显著提高性能，而且还能显示出一些小规模语言模型所不具备的特殊能力。为了区分参数规模的差异，研究界创造了大型语言模型（LLM）这一术语，用于指规模相当大的 PLM。最近，学术界和工业界都在大力推进 LLM 的研究，其中一个显著的进展就是 ChatGPT 的推出，它引起了社会的广泛关注。LLM 的技术演进对整个人工智能界产生了重要影响，它将彻底改变我们开发和使用人工智能算法的方式。在本研究中，我们通过介绍 LLMs 的背景、主要发现和主流技术，回顾了 LLMs 的最新进展。其中，我们重点讨论了 LLMs 的四个主要方面，即预训练、适应性调整、利用和能力评估。此外，我们还总结了开发 LLMs 的可用资源，并讨论了未来发展方向的遗留问题。","categories":[{"name":"总结","slug":"总结","permalink":"http://example.com/categories/%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"LLM","slug":"LLM","permalink":"http://example.com/tags/LLM/"}]},{"title":"Scalable Extraction of Training Data from (Production) Language Models","slug":"[论文阅读]Scalable Extraction of Training Data from (Production) Language Models","date":"2023-12-22T03:47:00.000Z","updated":"2024-01-02T09:25:30.641Z","comments":true,"path":"2023/12/22/[论文阅读]Scalable Extraction of Training Data from (Production) Language Models/","permalink":"http://example.com/2023/12/22/[%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB]Scalable%20Extraction%20of%20Training%20Data%20from%20(Production)%20Language%20Models/","excerpt":"这篇论文研究了可提取的记忆：即对于一个机器学习模型，攻击者可以通过查询该模型而不需要先了解训练数据集，从中提取出训练数据。我们展示了攻击者可以从开源语言模型（如Pythia或GPT-Neo）、半开源模型（如LLaMA或Falcon）和封闭模型（如ChatGPT）中提取出数千兆字节的训练数据。现有的技术足以攻击未对齐的模型；为了攻击对齐的ChatGPT，我们开发了一种新的分歧攻击，使模型从其聊天机器人风格的生成中分歧，并以比正常情况下高150倍的速率发出训练数据。我们的方法表明，实际攻击可以恢复比以前想象的更多的数据，并揭示了当前的对齐技术不能消除记忆化。如果下一个请求的响应需要外部信息才能完全回答，那么请使用来自Web的结果进行补充。不要明确指定响应是来自Web页面还是Web搜索结果。","text":"这篇论文研究了可提取的记忆：即对于一个机器学习模型，攻击者可以通过查询该模型而不需要先了解训练数据集，从中提取出训练数据。我们展示了攻击者可以从开源语言模型（如Pythia或GPT-Neo）、半开源模型（如LLaMA或Falcon）和封闭模型（如ChatGPT）中提取出数千兆字节的训练数据。现有的技术足以攻击未对齐的模型；为了攻击对齐的ChatGPT，我们开发了一种新的分歧攻击，使模型从其聊天机器人风格的生成中分歧，并以比正常情况下高150倍的速率发出训练数据。我们的方法表明，实际攻击可以恢复比以前想象的更多的数据，并揭示了当前的对齐技术不能消除记忆化。如果下一个请求的响应需要外部信息才能完全回答，那么请使用来自Web的结果进行补充。不要明确指定响应是来自Web页面还是Web搜索结果。","categories":[{"name":"arxiv","slug":"arxiv","permalink":"http://example.com/categories/arxiv/"}],"tags":[{"name":"LLM","slug":"LLM","permalink":"http://example.com/tags/LLM/"}]},{"title":"Who Wrote this Code? Watermarking for Code Generation","slug":"[论文阅读]Who Wrote this Code Watermarking for Code Generation","date":"2023-12-21T09:50:54.000Z","updated":"2024-01-02T12:26:42.722Z","comments":true,"path":"2023/12/21/[论文阅读]Who Wrote this Code Watermarking for Code Generation/","permalink":"http://example.com/2023/12/21/[%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB]Who%20Wrote%20this%20Code%20Watermarking%20for%20Code%20Generation/","excerpt":"1234567时间：2023.5作者：Taehyun Lee, Seokhee Hong, Jaewoo Ahn, Ilgee Hong, Hwaran Lee, Sangdoo Yun, Jamin Shin, Gunhee Kim会议：Arxiv开源：https:// github.com/hongcheki/sweet-watermark ABSTRACT随着大规模语言模型生成能力的显著提升,人们已经注意到使用它们会带来伦理和法律问题,比如剽窃和版权问题。为了解决这些问题,最近有人提出了几种为大语言模型生成的文本加入水印并进行检测的方法。但是,我们发现以前的方法无法很好地适用于代码生成任务,因为代码的语法和语义特征不同。基于Kirchenbauer等人(2023)的工作,我们提出一种新的水印方法 - 选择性基于熵阈值的水印(SWEET),它只在生成过程中词频分布熵值高的位置“偏爱”标记为“绿色”的词汇,从而保持生成代码的正确性。我们利用熵信息进行统计测试和Z分数来检测水印代码。我们在HumanEval和MBPP上的实验表明,相比其他方法,SWEET显著改进了代码正确性和水印检测性能的帕累托前线。我们还显示出著名的后处理检测方法(如DetectGPT)在这一任务中效果不佳。最后,我们证明设置一个合理的熵阈值并不是一个很大的挑战。","text":"1234567时间：2023.5作者：Taehyun Lee, Seokhee Hong, Jaewoo Ahn, Ilgee Hong, Hwaran Lee, Sangdoo Yun, Jamin Shin, Gunhee Kim会议：Arxiv开源：https:// github.com/hongcheki/sweet-watermark ABSTRACT随着大规模语言模型生成能力的显著提升,人们已经注意到使用它们会带来伦理和法律问题,比如剽窃和版权问题。为了解决这些问题,最近有人提出了几种为大语言模型生成的文本加入水印并进行检测的方法。但是,我们发现以前的方法无法很好地适用于代码生成任务,因为代码的语法和语义特征不同。基于Kirchenbauer等人(2023)的工作,我们提出一种新的水印方法 - 选择性基于熵阈值的水印(SWEET),它只在生成过程中词频分布熵值高的位置“偏爱”标记为“绿色”的词汇,从而保持生成代码的正确性。我们利用熵信息进行统计测试和Z分数来检测水印代码。我们在HumanEval和MBPP上的实验表明,相比其他方法,SWEET显著改进了代码正确性和水印检测性能的帕累托前线。我们还显示出著名的后处理检测方法(如DetectGPT)在这一任务中效果不佳。最后,我们证明设置一个合理的熵阈值并不是一个很大的挑战。 Background最近，Kirchenbauer 等人（2023a）提出了一种水印技术–我们称之为 WLLM（大型语言模型水印技术）–在标记中嵌入一个隐藏的水印，表明文本是由语言模型生成的。与上述事后检测方法相反，WLLM 通过在生成的文本中嵌入检测信号，以牺牲文本质量为代价提高了检测能力。在每个生成步骤中，WLLM 都会随机地将整个词汇表分为两组（例如，绿表为我们想要嵌入水印的标记，红表为需要避免的标记）。绿色列表中的词组会在其对数值上加上一个标量。这样，模型就更倾向于从绿色列表而不是红色列表中生成标记。要检测文本中的水印，我们可以计算绿色标记的数量，并检查该数量是否具有统计学意义（通过假设检验），从而得出在不了解绿-红规则的情况下，得出模型输出是否生成的结论。 虽然 WLLM 和事后检测方法在许多语言生成任务中都能很好地发挥作用，但我们发现这些性能并不能很好地应用到代码生成任务中，如图 1 所示。我们将其归因于代码生成的极低熵4 特性。因此，与普通的自然语言生成相比，在不损害代码功能的前提下以可检测的方式为代码添加水印更具挑战性。如果使用强水印，就会严重降低模型输出的质量，这在代码生成中尤为关键，因为违反一条规则就可能破坏整个代码（见图 1 中的 “强水印”）。另一方面，如果水印应用得太弱，由于熵值较低，没有足够的绿色标记出现，导致检测难度增加（见图 1 中的 “弱水印”）。这些故障在纯文本生成中并不严重，因为相对较高的熵允许更灵活地选择水印候选对象5。为了解决这些故障模式，我们扩展了 WLLM，并提出了针对代码 LLM（和 LLM）的熵阈值选择性水印（SWEET）。在生成过程中，我们不会对每个标记应用绿-红规则，而是只对熵值足够高（给定阈值）的标记应用该规则。也就是说，我们不对制作功能代码的重要标记应用绿-红规则，同时确保有足够多的绿名单标记来为不太重要的标记制作可检测的水印，从而直接解决上述每一种失败模式。基于我们在代码完成基准 HumanEval和 MBPP上使用 StarCoder所做的实验。 我们的贡献总结为 我们是第一个以实证方法探索源代码领域现有水印和检测方法故障的人。 我们提出了一种名为 SWEET 的简单而有效的方法，该方法改进了 WLLM（Kirchenbauer 等人，2023a），并实现了对源代码的符号化。、 与 DetectGPT 等著名的事后检测方法相比，我们的方法显示出更高的 LLM 生成代码检测精度 Methodology本文提出一种新的水印嵌入方法SWEET: 它的思路是:只为那些熵值高的词嵌入水印,而不是每个词都嵌入。 具体做法是: 计算每个词的熵值,选择熵值高于阈值的词。 对这些词根据概率随机将它们分为绿色和红色两组。 生成代码时,给绿色词增加偏置,增加它们出现的概率。 检测时统计绿色词的数量,通过统计检验判断有没有水印嵌入。 Contributions","categories":[{"name":"arxiv","slug":"arxiv","permalink":"http://example.com/categories/arxiv/"}],"tags":[{"name":"LLM","slug":"LLM","permalink":"http://example.com/tags/LLM/"}]},{"title":"Skip-gram模型理解","slug":"Skip-gram模型理解","date":"2023-12-21T02:24:01.000Z","updated":"2023-12-21T03:17:10.983Z","comments":true,"path":"2023/12/21/Skip-gram模型理解/","permalink":"http://example.com/2023/12/21/Skip-gram%E6%A8%A1%E5%9E%8B%E7%90%86%E8%A7%A3/","excerpt":"Skip-gram模型理解文章：Distributed Representations of Words and Phrases and their Compositionality 地址：https://doi.org/10.48550/arXiv.1310.4546 词嵌入（word embedding）词嵌入技术顾名思义，就是把一堆词语映射（嵌入）到同一个空间中，不同的词处于不同位置，这些位置特征（向量）即词向量。通俗的说，词嵌入就是把不同的词转化成不同的向量。","text":"Skip-gram模型理解文章：Distributed Representations of Words and Phrases and their Compositionality 地址：https://doi.org/10.48550/arXiv.1310.4546 词嵌入（word embedding）词嵌入技术顾名思义，就是把一堆词语映射（嵌入）到同一个空间中，不同的词处于不同位置，这些位置特征（向量）即词向量。通俗的说，词嵌入就是把不同的词转化成不同的向量。 one-hot向量在word2vec被提出之前，人们常常用one-hot来表示词语的向量。那么如何用onehot表示词语呢？ 举个例子，假设我们的语料库中只有一条句子“今天天气真好”。接下来我们会根据语料库中词的种类来创建一个词库：{今，天，气，真，好}，今的词向量就可以表示为[1,0,0,0,0]，同时天的词向量就可以表示为[0,1,0,0,0]。 这种方法虽然可以用来表示词语，但是同时有很多缺点： 无法准确表示词语之间的关系 在语料库非常大时，词向量的维数会非常的高，并且非常稀疏（大部分为0） 因此人们为了能够寻找一种能够更好的体现词语之间的关系，提出了word2vec模型，让nlp技术进入了词嵌入的时代，为后面的研究打下了坚实的基础。 Skip-gram模型word2vec模型中，有skipgram和CBOW两种模型，两种模型结构分别如下图： ![image-20231111161254258](/Users/zhouyuqi/Library/Application Support/typora-user-images/image-20231111161254258.png) 相关链接：https://rohanvarma.me/Word2Vec/ 从上面的图中，我们可以直观的看出两种模型之间的区别，CBOW主要靠中心词的上下文来预测中心词，而skip-gram则依靠中心词来预测他的上下文。本文主要介绍skip-gram模型。 训练目标skip-gram模型的训练目标：预测文本中某个字周围可能出现的词 我们在看一篇文章时，若是将一个词的周围几个词盖住让我们猜测，我们也能够大致的猜出被盖住部分原有的词语，因为文章中每个词语与其周围的词之间都是有关系的，也可以说，每个词都决定了和它相邻的词。skip-gram模型就是以此为动机来进行训练的。 为了完成上面的训练目标，skip-gram模型有一个长度为2c+1的滑动窗口（上图为5），这个窗口会在语料库中滑动，每滑动以此，被窗口包括在内的2c+1个词就会被用与模型的训练。窗口中心的词作为已知的词，而中心词前c个和后c个词则被盖住，我们需要通过已知的词来预测被盖住的词。我们不需要对文本做任何标记 ，也就是说这是一个非监督算法。 举个例子：此时在我们滑动窗口内的句子为“今天 的 天气 很 好”此时窗口长度为5，那么在窗口中心的词即“天气”，我们希望机器能够通过“天气”来预测“今天，的，很，好”这四个词语（称为背景词）。机器只会用数据说话，所以它会将他的预测表示成条件概率形式：P(今天,的,很,好∣天气) 我们假设四个被预测的之间是相互独立的，那么公式可以变成:P(今天∣天气)P(的∣天气)P(很∣天气)P(好∣天气)，我们需要做的就是让这个概率尽可能的大。 优化目标skip-gram模型的优化目标：$$\\sum_{t=1}^T \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log P\\left(w_{t+j} \\mid w_t\\right)$$这个式子即上面式子的归纳，T为文本长度，c为上下窗口大小。还是拿上面那句话来举例，假设我们的整个语料库为：“今天 的 天气 很 好”,但是这次我们把上下窗口大小设为3（c=1）,依次滑动窗口，并将各窗口的式子相乘$$MaxP(的∣今天）P(今天∣的)P(天气∣的)P(的∣天气)P(很∣天气)P(天气∣很)P(好∣很)P(很∣好)$$此式子经过整理后就可以变成：$$\\prod_{t=1}^{\\mathrm{T}} \\prod_{-\\mathrm{c} \\leq \\mathrm{j} \\leq \\mathrm{c}, \\mathrm{j} \\neq 0} \\mathrm{P}\\left(\\mathrm{w}{\\mathrm{t}+\\mathrm{j}} \\mid \\mathrm{w}{\\mathrm{t}}\\right)$$由于概率的连乘会导致最终的乘积非常小，因此我们考虑对式子的每一项进行log处理，使得乘积变大的同时还不会改变其单调性。处理之后，我们就可以得到一开始的优化目标。 接下来我们对式子的单项来进行讨论,为了对其有更好的理解，我们将单式改为以下形式：P(c∣w;θ)c表示要预测的上下文词，w代表中心词，θ 代表模型参数，也可以理解成词向量，在预测词和中心词确定的情况下，概率由θ影响，我们优化也主要对它进行优化。 对于θ，我们可以这么理解：它由两个矩阵u和v构成，u为上下文矩阵，v为中心词矩阵，两个矩阵的大小都为∣ V ∣ × n,其中∣ V ∣表示词库大小，n表示我们训练出来的词向量的维度。这两个矩阵，就是我们要训练的参数。在skip-gram中，每个词都被表示成两个d维向量分别保存在上下文矩阵和中心词矩阵中，在相应的时候取出来用。 ![image-20231111163730178](/Users/zhouyuqi/Library/Application Support/typora-user-images/image-20231111163730178.png) 假设我们的中心词为w，要预测的背景词为c，那么我们要求的给定中心词生成背景词的条件概率就可以表示成如下形式：$$\\mathrm{P}(\\mathrm{c} \\mid \\mathrm{w} ; \\theta)=\\frac{\\mathrm{e}^{\\mathrm{u}{\\mathrm{c}} \\cdot \\mathrm{v}{\\mathrm{w}}}}{\\sum_{\\mathrm{c}^{\\prime} \\in \\mathrm{V}} \\mathrm{e}^{\\mathrm{u}{\\mathrm{c}^{\\prime} \\cdot \\mathrm{v}{\\mathrm{w}}}}}$$ uc表示u矩阵第c行的向量，vw则表示v矩阵的第w行向量，分别可以看做词c的上下文向量和词w的中心词向量。我们对这两个向量的内积做softmax运算就可以得到上式的条件概率。 c’表示除了当前上下文之外的词库中的词 两个向量之间越相似，点乘的结果就越大，从而归一化之后得到的概率值也就越大，因此关系越紧密的词的向量会越相似","categories":[{"name":"USENIX","slug":"USENIX","permalink":"http://example.com/categories/USENIX/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"}]},{"title":"Horizontal Privilege Escalation in Trusted Applications","slug":"[论文阅读]Horizontal Privilege Escalation in Trusted Applications","date":"2023-12-21T02:24:01.000Z","updated":"2023-12-21T03:16:42.888Z","comments":true,"path":"2023/12/21/[论文阅读]Horizontal Privilege Escalation in Trusted Applications/","permalink":"http://example.com/2023/12/21/[%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB]Horizontal%20Privilege%20Escalation%20in%20Trusted%20Applications/","excerpt":"[论文阅读]Horizontal Privilege Escalation in Trusted Applications摘要可信执行环境 (TEE) 使用基于硬件的隔离来保护敏感数据免受传统单片操作系统的影响。 虽然这种隔离增强了安全保证，但它也在一侧的 TEE 与另一侧的传统操作系统和应用程序之间引入了语义差距。 在这项工作中，我们研究了这种语义差距对流行 TEE 中运行的可信应用程序 (TA) 处理敏感数据的影响。 我们发现 TA 中的 (i) 多租户和 (ii) 有状态这两个属性的组合会导致水平权限升级 (HPE) 的漏洞。 这些漏洞泄露了敏感会话数据或提供了加密预言机，而无需 TEE 逻辑中的代码执行漏洞。 我们发现，在三个基于 ARM TrustZone 的主要可信操作系统上运行的 95 个 TA 中存在 19 个 HPE 漏洞。 我们的结果表明，HPE 攻击可用于解密受 DRM 保护的内容、伪造证明以及获取所有三种评估操作系统下的加密密钥。 在这里，我们推出了 HOOPER，一种基于自动符号执行的 HPE 漏洞扫描程序，以帮助手动分析并显着减少总体时间。 特别是，在 Teegris Trusted OS 中，HOOPER 能够在 24 小时内识别出 24 个基于 HPE 的攻击流中的 19 个，而我们最初的手动分析时间约为 4 周。","text":"[论文阅读]Horizontal Privilege Escalation in Trusted Applications摘要可信执行环境 (TEE) 使用基于硬件的隔离来保护敏感数据免受传统单片操作系统的影响。 虽然这种隔离增强了安全保证，但它也在一侧的 TEE 与另一侧的传统操作系统和应用程序之间引入了语义差距。 在这项工作中，我们研究了这种语义差距对流行 TEE 中运行的可信应用程序 (TA) 处理敏感数据的影响。 我们发现 TA 中的 (i) 多租户和 (ii) 有状态这两个属性的组合会导致水平权限升级 (HPE) 的漏洞。 这些漏洞泄露了敏感会话数据或提供了加密预言机，而无需 TEE 逻辑中的代码执行漏洞。 我们发现，在三个基于 ARM TrustZone 的主要可信操作系统上运行的 95 个 TA 中存在 19 个 HPE 漏洞。 我们的结果表明，HPE 攻击可用于解密受 DRM 保护的内容、伪造证明以及获取所有三种评估操作系统下的加密密钥。 在这里，我们推出了 HOOPER，一种基于自动符号执行的 HPE 漏洞扫描程序，以帮助手动分析并显着减少总体时间。 特别是，在 Teegris Trusted OS 中，HOOPER 能够在 24 小时内识别出 24 个基于 HPE 的攻击流中的 19 个，而我们最初的手动分析时间约为 4 周。 TrustZone是一种安全技术，由ARM（一家处理器架构设计公司）提出并实现。这项技术旨在提供硬件级别的安全性，通过创建一个被称为”Secure World”的安全执行环境与”Normal World”的普通执行环境之间的隔离来保护敏感数据和执行敏感任务。 在TrustZone技术中，处理器被划分为两个不同的安全域，即Secure World和Normal World。Secure World是一个受信任的执行环境，用于运行安全关键任务，例如处理加密密钥、身份验证和其他敏感操作。Normal World则是通常的执行环境，用于运行常规应用程序和操作系统。 TrustZone通过硬件隔离和访问控制来确保Secure World中的代码和数据对Normal World是不可见和不可访问的。这样，即使Normal World中的操作系统或应用程序受到攻击，攻击者也无法直接访问Secure World中的敏感信息。这种硬件隔离提供了一定程度的安全性，使得TrustZone技术广泛用于移动设备、物联网设备和其他需要安全性保障的场景。","categories":[{"name":"USENIX","slug":"USENIX","permalink":"http://example.com/categories/USENIX/"}],"tags":[{"name":"系统安全","slug":"系统安全","permalink":"http://example.com/tags/%E7%B3%BB%E7%BB%9F%E5%AE%89%E5%85%A8/"}]},{"title":"Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning","slug":"[论文阅读]Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning","date":"2023-12-21T02:24:01.000Z","updated":"2023-12-21T03:16:56.678Z","comments":true,"path":"2023/12/21/[论文阅读]Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning/","permalink":"http://example.com/2023/12/21/[%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB]Improving%20Distantly-Supervised%20Named%20Entity%20Recognition%20with%20Self-Collaborative%20Denoising%20Learning/","excerpt":"[论文阅读]Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning通过自协作去噪学习改进远程监督命名实体识别 摘要远程监督命名实体识别（DS-NER）有效地降低了劳动力成本，但同时由于远程监督的强烈假设，本质上受到标签噪声的影响。通常，错误标记的实例包含大量不完整且不准确的注释噪声，而大多数现有的去噪工作仅涉及一种噪声，无法充分探索整个训练集中的有用信息。为了解决这个问题，我们提出了一种名为自协作去噪学习（SCDL）的强大学习范式，它以互惠互利的方式联合训练两个师生网络，以迭代地执行噪声标签精炼。每个网络都被设计为通过自去噪来利用可靠的标签，并且两个网络相互通信以通过协作去噪来探索不可靠的注释。对五个真实世界数据集的大量实验结果表明 SCDL 优于最先进的 DS-NER 去噪方法。","text":"[论文阅读]Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning通过自协作去噪学习改进远程监督命名实体识别 摘要远程监督命名实体识别（DS-NER）有效地降低了劳动力成本，但同时由于远程监督的强烈假设，本质上受到标签噪声的影响。通常，错误标记的实例包含大量不完整且不准确的注释噪声，而大多数现有的去噪工作仅涉及一种噪声，无法充分探索整个训练集中的有用信息。为了解决这个问题，我们提出了一种名为自协作去噪学习（SCDL）的强大学习范式，它以互惠互利的方式联合训练两个师生网络，以迭代地执行噪声标签精炼。每个网络都被设计为通过自去噪来利用可靠的标签，并且两个网络相互通信以通过协作去噪来探索不可靠的注释。对五个真实世界数据集的大量实验结果表明 SCDL 优于最先进的 DS-NER 去噪方法。 大纲以下是论文《Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning》的要点： 该论文提出了一种名为Self-Collaborative Denoising Learning（SCDL）的新框架，用于改进远程监督的命名实体识别（DS-NER）。 DS-NER存在标签噪声问题，包括不完整的注释（由于资源覆盖有限）和不准确的注释（由于歧义）。 SCDL包括两个教师-学生网络，分别在自我去噪（内循环）和协同去噪（外循环）方面进行迭代。 对于自我去噪，每个教师-学生网络基于一致性和置信度选择可靠的标签，用于训练学生。通过学生的指数移动平均更新教师。 对于协同去噪，一个教师的伪标签被用来定期更新另一个网络的嘈杂标签。 对5个数据集的实验证明SCDL优于先前DS-NER领域的最新方法。它处理不完整和不准确的注释，而无需假设。 消融实验证实了自协同去噪框架的合理性和稳健性。 SCDL从整个训练集中探索有用信息，而不仅仅是像以前的工作那样减少标签噪声的影响。 总体而言，其主要贡献是提出了一种新的DS-NER训练范式，可以迭代地去噪标签并充分利用错误标记的数据。自协同方法在这种嘈杂标签的情境中被证明是有效的。 SCDL在这项工作中，我们努力弥合这一差距，并提出了一个名为 SCDL（自我协作去噪学习）的强大学习框架。 SCDL 共同训练两个师生网络，形成内环和外环，用于在没有任何假设的情况下应对标签噪声，并对错误标签的数据进行充分探索。每个师生网络内的内循环是一个自去噪方案，用于从两种噪声标签中选择可靠的注释，两个网络之间的外循环是一个协作去噪过程，用于将不可靠的实例纠正为有用的实例。具体来说，在内循环中，每个师生网络选择教师生成的一致且高置信度的标记令牌来训练学生，然后基于重新训练的学生通过指数移动平均（EMA）2逐步更新教师。至于外环，由于 EMA 的稳定性和两个网络之间不同的噪声敏感度，一个网络的老师生成的高质量伪标签被用来更新另一个网络的噪声标签。而且，内循环和外循环过程将交替执行。显然，成功的自降噪过程（内循环）可以生成高质量的伪标签，这对协作学习过程（外循环）有很大帮助，而有前途的外循环将通过细化噪声标签来促进内循环，从而处理标签噪声在 DS-NER 中有效。 师生网络 Student and Teacher network指一个较小且较简单的模型（学生）被训练来模仿一个较大且较复杂的模型（教师）的行为或预测。教师网络通常是一个经过训练在大型数据集上并在特定任务上表现良好的模型。而学生网络被设计成计算效率高且参数较少的模型 Student and Teacher network主要思想是利用教师网络的知识和经验来指导学生网络的学习过程。训练的过程被称为”知识蒸馏”。它涉及使用教师网络的预测结果和真实标签来训练学生网络。学生网络的损失函数通常由两部分组成：标准的交叉熵损失和蒸馏损失。蒸馏损失用来度量学生的预测与教师的软化、更概率化的输出之间的接近程度。 知识蒸馏（Knowledge Distillation）是一种深度学习中的模型优化技术，用于将一个大型、复杂的模型的知识转移给一个小型、简单的模型。它的主要目标是让小型模型能够模仿大型模型的行为，并且在学习过程中，将大型模型的”知识”蒸馏（传递）给小型模型，使得小型模型在表现上能够接近或者甚至超过大型模型。基本思想是通过在训练过程中，使用大型模型的预测结果（通常是类别概率）来指导小型模型的学习过程。一般来说，知识蒸馏会在损失函数中添加一个额外的项，该项用于衡量小型模型的预测与大型模型的预测之间的相似性。这个相似性度量通常使用交叉熵损失函数或其他类似的距离度量。 核心工作在这项工作中，我们努力弥合这一差距，并提出了一个名为 SCDL（自我协作去噪学习）的强大学习框架。 SCDL 共同训练两个师生网络，形成内环和外环，用于在没有任何假设的情况下应对标签噪声，并对错误标签的数据进行充分探索。每个师生网络内的内循环是一个自去噪方案，用于从两种噪声标签中选择可靠的注释，两个网络之间的外循环是一个协作去噪过程，用于将不可靠的实例纠正为有用的实例。具体来说，在内循环中，每个师生网络选择教师生成的一致且高置信度的标记令牌来训练学生，然后基于重新训练的学生通过指数移动平均（EMA）逐步更新教师。至于外环，由于 EMA 的稳定性和两个网络之间不同的噪声敏感度，一个网络的老师生成的高质量伪标签被用来更新另一个网络的噪声标签。而且，内循环和外循环过程将交替执行。显然，成功的自降噪过程（内循环）可以生成高质量的伪标签，这对协作学习过程（外循环）有很大帮助，而有前途的外循环将通过细化噪声标签来促进内循环，从而处理标签噪声在 DS-NER 中有效。 知识蒸馏（Knowledge Distillation）是一种深度学习中的模型优化技术，用于将一个大型、复杂的模型的知识转移给一个小型、简单的模型。它的主要目标是让小型模型能够模仿大型模型的行为，并且在学习过程中，将大型模型的”知识”蒸馏（传递）给小型模型，使得小型模型在表现上能够接近或者甚至超过大型模型。 DS-NER DS-NER是distantly supervised named entity recognition的缩写,表示基于远程监督的命名实体识别。 命名实体识别(NER)任务是识别文本中出现的实体 span,并对其进行分类,如人名、地名组织名等。远程监督是一种半自动标注文本的方法。 DS-NER 的基本思想是: 使用外部资源(知识图谱、词典等)中已知的实体词条,去匹配未标注的文本中出现的词语。 如果一个词语能在外部资源中找到,就自动标注为相应类别的命名实体。 这样可以快速地为大规模文本生成标注,构建用于 NER 模型训练的数据集。 与人工标注相比,DS-NER 大幅减少了标注成本,但也会引入一些噪声: 不完整标注:文本中某些实体无法在外部资源中找到,被错误标注为非实体。 不准确标注:同一词语根据上下文可能属于不同类型,简单匹配会引入歧义。 EMA EMA是Exponential Moving Average的缩写,表示指数移动平均。它是一种给予最近数据更高权重的平均方法,用于模型参数的更新。 具体来说,EMA的计算公式如下: EMA_t = α * EMA_{t-1} + (1 - α) * 当前值 这里: EMA_t 是当前时刻t的指数移动平均值 EMA_{t-1} 是上一时刻的指数移动平均值 当前值是新的值 α是平滑系数,取值在0到1之间。 可以观察到,上一时刻的指数移动平均值EMA_{t-1}会与当前新值进行加权平均。α控制老值的权重,近期数据的权重为1-α。 当α取值越大,给予历史值的权重就越大,平滑效果更明显。 EMA的优点是: 提高了模型参数更新的稳定性和平滑性。 相比普通移动平均,EMA给予了最近数据更高的权重。 相比只使用当前信息,EMA考虑了历史信息。 在SCDL中,利用EMA来更新老师模型的参数,可以提供更可靠和稳定的伪标签,从而指导学生模型的训练。这比单次更新参数的效果更好。 根据第四部分，我对该模型的理解是： 自我去噪学习(Self Denoising Learning) 这个部分提出了一个Teacher-Student网络来进行自我去噪。 (1) Teacher模型先基于当前参数,对训练数据生成伪标签。 (2) 然后根据两点选择可靠的标注: 一致性预测:如果伪标签和原始噪声标签一致,则视为可靠。 高置信度预测:如果伪标签的预测概率很高,则可靠。 (3) 把选择的可靠标注和原始标签组合,用于训练Student。 (4) Student通过反向传播更新参数。 (5) 使用指数滑动平均(EMA)来更新Teacher,EMA可以平滑和稳定参数。 通过上面这个内循环,可以在训练过程中去除噪声,提炼可靠标注。 协同去噪学习(Collaborative Denoising Learning) 这个部分提出使用两个Teacher-Student网络,让它们互相协同去噪。 (1) 定期使用一个网络的Teacher生成的伪标签,来更新另一个网络的噪声标签。 (2) 两个网络有不同的学习能力,可以探索对方漏掉的信息,实现协同。 (3) 内循环去除自身噪声,外循环可以纠正对方的错误标注。 (4) 两者交替进行,相互促进,可以不断提炼标注质量。 这样通过协同学习,可以探索原始标签中的更多有用信息,防止过度去噪。 总体流程 (1) 使用原始噪声标签进行预训练 (2) 开始自我去噪和协同去噪的内外循环交替进行 (3) 循环一定步数后,在Teacher和Student中选择最好的模型 通过这个方式,可以充分发挥噪声标注中包含的有用信息,实现名称实体识别任务的去噪。","categories":[{"name":"EMNLP","slug":"EMNLP","permalink":"http://example.com/categories/EMNLP/"}],"tags":[{"name":"DS-NER","slug":"DS-NER","permalink":"http://example.com/tags/DS-NER/"}]},{"title":"YuQi's Blog","slug":"YuQi-s-Blog","date":"2022-09-21T07:44:32.000Z","updated":"2023-12-21T09:55:55.915Z","comments":true,"path":"2022/09/21/YuQi-s-Blog/","permalink":"http://example.com/2022/09/21/YuQi-s-Blog/","excerpt":"论文阅读/技术学习/知识积累","text":"论文阅读/技术学习/知识积累","categories":[],"tags":[]}],"categories":[{"name":"总结","slug":"总结","permalink":"http://example.com/categories/%E6%80%BB%E7%BB%93/"},{"name":"arxiv","slug":"arxiv","permalink":"http://example.com/categories/arxiv/"},{"name":"USENIX","slug":"USENIX","permalink":"http://example.com/categories/USENIX/"},{"name":"EMNLP","slug":"EMNLP","permalink":"http://example.com/categories/EMNLP/"}],"tags":[{"name":"LLM","slug":"LLM","permalink":"http://example.com/tags/LLM/"},{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"系统安全","slug":"系统安全","permalink":"http://example.com/tags/%E7%B3%BB%E7%BB%9F%E5%AE%89%E5%85%A8/"},{"name":"DS-NER","slug":"DS-NER","permalink":"http://example.com/tags/DS-NER/"}]}