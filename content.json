{"meta":{"title":"RainYu","subtitle":"","description":"","author":"RainYu","url":"http://example.com","root":"/"},"pages":[{"title":"about","date":"2023-12-21T02:21:44.000Z","updated":"2023-12-21T02:21:44.628Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":""},{"title":"tags","date":"2023-12-21T02:22:05.000Z","updated":"2023-12-21T02:23:08.584Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2023-12-21T02:21:55.000Z","updated":"2023-12-21T02:22:44.584Z","comments":false,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"codellama","slug":"codellama","date":"2024-01-02T02:09:45.000Z","updated":"2024-01-02T03:11:21.887Z","comments":true,"path":"2024/01/02/codellama/","permalink":"http://example.com/2024/01/02/codellama/","excerpt":"","text":"","categories":[{"name":"arxiv","slug":"arxiv","permalink":"http://example.com/categories/arxiv/"}],"tags":[{"name":"综述","slug":"综述","permalink":"http://example.com/tags/%E7%BB%BC%E8%BF%B0/"}]},{"title":"LLM综述","slug":"LLM综述","date":"2023-12-22T07:06:14.000Z","updated":"2023-12-22T07:19:25.986Z","comments":true,"path":"2023/12/22/LLM综述/","permalink":"http://example.com/2023/12/22/LLM%E7%BB%BC%E8%BF%B0/","excerpt":"语言本质上是一个由人类表达方式组成的复杂、错综复杂的系统，受语法规则的制约。开发能够理解和掌握语言的人工智能算法是一项重大挑战。作为一种主要方法，语言建模在过去二十年中被广泛用于语言理解和生成，从统计语言模型发展到神经语言模型。最近，预训练语言模型（PLMs）通过在大规模语料库中预训练 Transformer 模型而被提出，在解决各种 NLP 任务方面显示出强大的能力。由于研究人员发现模型缩放可以提高性能，他们进一步研究了缩放效应，将模型规模扩大到更大。有趣的是，当参数比例超过一定水平时，这些扩大的语言模型不仅能显著提高性能，而且还能显示出一些小规模语言模型所不具备的特殊能力。为了区分参数规模的差异，研究界创造了大型语言模型（LLM）这一术语，用于指规模相当大的 PLM。最近，学术界和工业界都在大力推进 LLM 的研究，其中一个显著的进展就是 ChatGPT 的推出，它引起了社会的广泛关注。LLM 的技术演进对整个人工智能界产生了重要影响，它将彻底改变我们开发和使用人工智能算法的方式。在本研究中，我们通过介绍 LLMs 的背景、主要发现和主流技术，回顾了 LLMs 的最新进展。其中，我们重点讨论了 LLMs 的四个主要方面，即预训练、适应性调整、利用和能力评估。此外，我们还总结了开发 LLMs 的可用资源，并讨论了未来发展方向的遗留问题。","text":"语言本质上是一个由人类表达方式组成的复杂、错综复杂的系统，受语法规则的制约。开发能够理解和掌握语言的人工智能算法是一项重大挑战。作为一种主要方法，语言建模在过去二十年中被广泛用于语言理解和生成，从统计语言模型发展到神经语言模型。最近，预训练语言模型（PLMs）通过在大规模语料库中预训练 Transformer 模型而被提出，在解决各种 NLP 任务方面显示出强大的能力。由于研究人员发现模型缩放可以提高性能，他们进一步研究了缩放效应，将模型规模扩大到更大。有趣的是，当参数比例超过一定水平时，这些扩大的语言模型不仅能显著提高性能，而且还能显示出一些小规模语言模型所不具备的特殊能力。为了区分参数规模的差异，研究界创造了大型语言模型（LLM）这一术语，用于指规模相当大的 PLM。最近，学术界和工业界都在大力推进 LLM 的研究，其中一个显著的进展就是 ChatGPT 的推出，它引起了社会的广泛关注。LLM 的技术演进对整个人工智能界产生了重要影响，它将彻底改变我们开发和使用人工智能算法的方式。在本研究中，我们通过介绍 LLMs 的背景、主要发现和主流技术，回顾了 LLMs 的最新进展。其中，我们重点讨论了 LLMs 的四个主要方面，即预训练、适应性调整、利用和能力评估。此外，我们还总结了开发 LLMs 的可用资源，并讨论了未来发展方向的遗留问题。","categories":[{"name":"总结","slug":"总结","permalink":"http://example.com/categories/%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"LLM","slug":"LLM","permalink":"http://example.com/tags/LLM/"}]},{"title":"Scalable Extraction of Training Data from (Production) Language Models","slug":"[论文阅读]Scalable Extraction of Training Data from (Production) Language Models","date":"2023-12-22T03:47:00.000Z","updated":"2024-01-02T09:25:30.641Z","comments":true,"path":"2023/12/22/[论文阅读]Scalable Extraction of Training Data from (Production) Language Models/","permalink":"http://example.com/2023/12/22/[%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB]Scalable%20Extraction%20of%20Training%20Data%20from%20(Production)%20Language%20Models/","excerpt":"这篇论文研究了可提取的记忆：即对于一个机器学习模型，攻击者可以通过查询该模型而不需要先了解训练数据集，从中提取出训练数据。我们展示了攻击者可以从开源语言模型（如Pythia或GPT-Neo）、半开源模型（如LLaMA或Falcon）和封闭模型（如ChatGPT）中提取出数千兆字节的训练数据。现有的技术足以攻击未对齐的模型；为了攻击对齐的ChatGPT，我们开发了一种新的分歧攻击，使模型从其聊天机器人风格的生成中分歧，并以比正常情况下高150倍的速率发出训练数据。我们的方法表明，实际攻击可以恢复比以前想象的更多的数据，并揭示了当前的对齐技术不能消除记忆化。如果下一个请求的响应需要外部信息才能完全回答，那么请使用来自Web的结果进行补充。不要明确指定响应是来自Web页面还是Web搜索结果。","text":"这篇论文研究了可提取的记忆：即对于一个机器学习模型，攻击者可以通过查询该模型而不需要先了解训练数据集，从中提取出训练数据。我们展示了攻击者可以从开源语言模型（如Pythia或GPT-Neo）、半开源模型（如LLaMA或Falcon）和封闭模型（如ChatGPT）中提取出数千兆字节的训练数据。现有的技术足以攻击未对齐的模型；为了攻击对齐的ChatGPT，我们开发了一种新的分歧攻击，使模型从其聊天机器人风格的生成中分歧，并以比正常情况下高150倍的速率发出训练数据。我们的方法表明，实际攻击可以恢复比以前想象的更多的数据，并揭示了当前的对齐技术不能消除记忆化。如果下一个请求的响应需要外部信息才能完全回答，那么请使用来自Web的结果进行补充。不要明确指定响应是来自Web页面还是Web搜索结果。","categories":[{"name":"arxiv","slug":"arxiv","permalink":"http://example.com/categories/arxiv/"}],"tags":[{"name":"LLM","slug":"LLM","permalink":"http://example.com/tags/LLM/"}]},{"title":"Skip-gram模型理解","slug":"Skip-gram模型理解","date":"2023-12-21T02:24:01.000Z","updated":"2023-12-21T03:17:10.983Z","comments":true,"path":"2023/12/21/Skip-gram模型理解/","permalink":"http://example.com/2023/12/21/Skip-gram%E6%A8%A1%E5%9E%8B%E7%90%86%E8%A7%A3/","excerpt":"Skip-gram模型理解文章：Distributed Representations of Words and Phrases and their Compositionality 地址：https://doi.org/10.48550/arXiv.1310.4546 词嵌入（word embedding）词嵌入技术顾名思义，就是把一堆词语映射（嵌入）到同一个空间中，不同的词处于不同位置，这些位置特征（向量）即词向量。通俗的说，词嵌入就是把不同的词转化成不同的向量。","text":"Skip-gram模型理解文章：Distributed Representations of Words and Phrases and their Compositionality 地址：https://doi.org/10.48550/arXiv.1310.4546 词嵌入（word embedding）词嵌入技术顾名思义，就是把一堆词语映射（嵌入）到同一个空间中，不同的词处于不同位置，这些位置特征（向量）即词向量。通俗的说，词嵌入就是把不同的词转化成不同的向量。 one-hot向量在word2vec被提出之前，人们常常用one-hot来表示词语的向量。那么如何用onehot表示词语呢？ 举个例子，假设我们的语料库中只有一条句子“今天天气真好”。接下来我们会根据语料库中词的种类来创建一个词库：{今，天，气，真，好}，今的词向量就可以表示为[1,0,0,0,0]，同时天的词向量就可以表示为[0,1,0,0,0]。 这种方法虽然可以用来表示词语，但是同时有很多缺点： 无法准确表示词语之间的关系 在语料库非常大时，词向量的维数会非常的高，并且非常稀疏（大部分为0） 因此人们为了能够寻找一种能够更好的体现词语之间的关系，提出了word2vec模型，让nlp技术进入了词嵌入的时代，为后面的研究打下了坚实的基础。 Skip-gram模型word2vec模型中，有skipgram和CBOW两种模型，两种模型结构分别如下图： ![image-20231111161254258](&#x2F;Users&#x2F;zhouyuqi&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20231111161254258.png) 相关链接：https://rohanvarma.me/Word2Vec/ 从上面的图中，我们可以直观的看出两种模型之间的区别，CBOW主要靠中心词的上下文来预测中心词，而skip-gram则依靠中心词来预测他的上下文。本文主要介绍skip-gram模型。 训练目标skip-gram模型的训练目标：预测文本中某个字周围可能出现的词 我们在看一篇文章时，若是将一个词的周围几个词盖住让我们猜测，我们也能够大致的猜出被盖住部分原有的词语，因为文章中每个词语与其周围的词之间都是有关系的，也可以说，每个词都决定了和它相邻的词。skip-gram模型就是以此为动机来进行训练的。 为了完成上面的训练目标，skip-gram模型有一个长度为2c+1的滑动窗口（上图为5），这个窗口会在语料库中滑动，每滑动以此，被窗口包括在内的2c+1个词就会被用与模型的训练。窗口中心的词作为已知的词，而中心词前c个和后c个词则被盖住，我们需要通过已知的词来预测被盖住的词。我们不需要对文本做任何标记 ，也就是说这是一个非监督算法。 举个例子：此时在我们滑动窗口内的句子为“今天 的 天气 很 好”此时窗口长度为5，那么在窗口中心的词即“天气”，我们希望机器能够通过“天气”来预测“今天，的，很，好”这四个词语（称为背景词）。机器只会用数据说话，所以它会将他的预测表示成条件概率形式：P(今天,的,很,好∣天气) 我们假设四个被预测的之间是相互独立的，那么公式可以变成:P(今天∣天气)P(的∣天气)P(很∣天气)P(好∣天气)，我们需要做的就是让这个概率尽可能的大。 优化目标skip-gram模型的优化目标：$$\\sum_{t&#x3D;1}^T \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log P\\left(w_{t+j} \\mid w_t\\right)$$这个式子即上面式子的归纳，T为文本长度，c为上下窗口大小。还是拿上面那句话来举例，假设我们的整个语料库为：“今天 的 天气 很 好”,但是这次我们把上下窗口大小设为3（c&#x3D;1）,依次滑动窗口，并将各窗口的式子相乘$$MaxP(的∣今天）P(今天∣的)P(天气∣的)P(的∣天气)P(很∣天气)P(天气∣很)P(好∣很)P(很∣好)$$此式子经过整理后就可以变成：$$\\prod_{t&#x3D;1}^{\\mathrm{T}} \\prod_{-\\mathrm{c} \\leq \\mathrm{j} \\leq \\mathrm{c}, \\mathrm{j} \\neq 0} \\mathrm{P}\\left(\\mathrm{w}{\\mathrm{t}+\\mathrm{j}} \\mid \\mathrm{w}{\\mathrm{t}}\\right)$$由于概率的连乘会导致最终的乘积非常小，因此我们考虑对式子的每一项进行log处理，使得乘积变大的同时还不会改变其单调性。处理之后，我们就可以得到一开始的优化目标。 接下来我们对式子的单项来进行讨论,为了对其有更好的理解，我们将单式改为以下形式：P(c∣w;θ)c表示要预测的上下文词，w代表中心词，θ 代表模型参数，也可以理解成词向量，在预测词和中心词确定的情况下，概率由θ影响，我们优化也主要对它进行优化。 对于θ，我们可以这么理解：它由两个矩阵u和v构成，u为上下文矩阵，v为中心词矩阵，两个矩阵的大小都为∣ V ∣ × n,其中∣ V ∣表示词库大小，n表示我们训练出来的词向量的维度。这两个矩阵，就是我们要训练的参数。在skip-gram中，每个词都被表示成两个d维向量分别保存在上下文矩阵和中心词矩阵中，在相应的时候取出来用。 ![image-20231111163730178](&#x2F;Users&#x2F;zhouyuqi&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20231111163730178.png) 假设我们的中心词为w，要预测的背景词为c，那么我们要求的给定中心词生成背景词的条件概率就可以表示成如下形式：$$\\mathrm{P}(\\mathrm{c} \\mid \\mathrm{w} ; \\theta)&#x3D;\\frac{\\mathrm{e}^{\\mathrm{u}{\\mathrm{c}} \\cdot \\mathrm{v}{\\mathrm{w}}}}{\\sum_{\\mathrm{c}^{\\prime} \\in \\mathrm{V}} \\mathrm{e}^{\\mathrm{u}{\\mathrm{c}^{\\prime} \\cdot \\mathrm{v}{\\mathrm{w}}}}}$$ uc表示u矩阵第c行的向量，vw则表示v矩阵的第w行向量，分别可以看做词c的上下文向量和词w的中心词向量。我们对这两个向量的内积做softmax运算就可以得到上式的条件概率。 c’表示除了当前上下文之外的词库中的词 两个向量之间越相似，点乘的结果就越大，从而归一化之后得到的概率值也就越大，因此关系越紧密的词的向量会越相似","categories":[{"name":"USENIX","slug":"USENIX","permalink":"http://example.com/categories/USENIX/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"}]},{"title":"Horizontal Privilege Escalation in Trusted Applications","slug":"[论文阅读]Horizontal Privilege Escalation in Trusted Applications","date":"2023-12-21T02:24:01.000Z","updated":"2023-12-21T03:16:42.888Z","comments":true,"path":"2023/12/21/[论文阅读]Horizontal Privilege Escalation in Trusted Applications/","permalink":"http://example.com/2023/12/21/[%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB]Horizontal%20Privilege%20Escalation%20in%20Trusted%20Applications/","excerpt":"[论文阅读]Horizontal Privilege Escalation in Trusted Applications摘要可信执行环境 (TEE) 使用基于硬件的隔离来保护敏感数据免受传统单片操作系统的影响。 虽然这种隔离增强了安全保证，但它也在一侧的 TEE 与另一侧的传统操作系统和应用程序之间引入了语义差距。 在这项工作中，我们研究了这种语义差距对流行 TEE 中运行的可信应用程序 (TA) 处理敏感数据的影响。 我们发现 TA 中的 (i) 多租户和 (ii) 有状态这两个属性的组合会导致水平权限升级 (HPE) 的漏洞。 这些漏洞泄露了敏感会话数据或提供了加密预言机，而无需 TEE 逻辑中的代码执行漏洞。 我们发现，在三个基于 ARM TrustZone 的主要可信操作系统上运行的 95 个 TA 中存在 19 个 HPE 漏洞。 我们的结果表明，HPE 攻击可用于解密受 DRM 保护的内容、伪造证明以及获取所有三种评估操作系统下的加密密钥。 在这里，我们推出了 HOOPER，一种基于自动符号执行的 HPE 漏洞扫描程序，以帮助手动分析并显着减少总体时间。 特别是，在 Teegris Trusted OS 中，HOOPER 能够在 24 小时内识别出 24 个基于 HPE 的攻击流中的 19 个，而我们最初的手动分析时间约为 4 周。","text":"[论文阅读]Horizontal Privilege Escalation in Trusted Applications摘要可信执行环境 (TEE) 使用基于硬件的隔离来保护敏感数据免受传统单片操作系统的影响。 虽然这种隔离增强了安全保证，但它也在一侧的 TEE 与另一侧的传统操作系统和应用程序之间引入了语义差距。 在这项工作中，我们研究了这种语义差距对流行 TEE 中运行的可信应用程序 (TA) 处理敏感数据的影响。 我们发现 TA 中的 (i) 多租户和 (ii) 有状态这两个属性的组合会导致水平权限升级 (HPE) 的漏洞。 这些漏洞泄露了敏感会话数据或提供了加密预言机，而无需 TEE 逻辑中的代码执行漏洞。 我们发现，在三个基于 ARM TrustZone 的主要可信操作系统上运行的 95 个 TA 中存在 19 个 HPE 漏洞。 我们的结果表明，HPE 攻击可用于解密受 DRM 保护的内容、伪造证明以及获取所有三种评估操作系统下的加密密钥。 在这里，我们推出了 HOOPER，一种基于自动符号执行的 HPE 漏洞扫描程序，以帮助手动分析并显着减少总体时间。 特别是，在 Teegris Trusted OS 中，HOOPER 能够在 24 小时内识别出 24 个基于 HPE 的攻击流中的 19 个，而我们最初的手动分析时间约为 4 周。 TrustZone是一种安全技术，由ARM（一家处理器架构设计公司）提出并实现。这项技术旨在提供硬件级别的安全性，通过创建一个被称为”Secure World”的安全执行环境与”Normal World”的普通执行环境之间的隔离来保护敏感数据和执行敏感任务。 在TrustZone技术中，处理器被划分为两个不同的安全域，即Secure World和Normal World。Secure World是一个受信任的执行环境，用于运行安全关键任务，例如处理加密密钥、身份验证和其他敏感操作。Normal World则是通常的执行环境，用于运行常规应用程序和操作系统。 TrustZone通过硬件隔离和访问控制来确保Secure World中的代码和数据对Normal World是不可见和不可访问的。这样，即使Normal World中的操作系统或应用程序受到攻击，攻击者也无法直接访问Secure World中的敏感信息。这种硬件隔离提供了一定程度的安全性，使得TrustZone技术广泛用于移动设备、物联网设备和其他需要安全性保障的场景。","categories":[{"name":"USENIX","slug":"USENIX","permalink":"http://example.com/categories/USENIX/"}],"tags":[{"name":"系统安全","slug":"系统安全","permalink":"http://example.com/tags/%E7%B3%BB%E7%BB%9F%E5%AE%89%E5%85%A8/"}]},{"title":"Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning","slug":"[论文阅读]Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning","date":"2023-12-21T02:24:01.000Z","updated":"2023-12-21T03:16:56.678Z","comments":true,"path":"2023/12/21/[论文阅读]Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning/","permalink":"http://example.com/2023/12/21/[%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB]Improving%20Distantly-Supervised%20Named%20Entity%20Recognition%20with%20Self-Collaborative%20Denoising%20Learning/","excerpt":"[论文阅读]Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning通过自协作去噪学习改进远程监督命名实体识别 摘要远程监督命名实体识别（DS-NER）有效地降低了劳动力成本，但同时由于远程监督的强烈假设，本质上受到标签噪声的影响。通常，错误标记的实例包含大量不完整且不准确的注释噪声，而大多数现有的去噪工作仅涉及一种噪声，无法充分探索整个训练集中的有用信息。为了解决这个问题，我们提出了一种名为自协作去噪学习（SCDL）的强大学习范式，它以互惠互利的方式联合训练两个师生网络，以迭代地执行噪声标签精炼。每个网络都被设计为通过自去噪来利用可靠的标签，并且两个网络相互通信以通过协作去噪来探索不可靠的注释。对五个真实世界数据集的大量实验结果表明 SCDL 优于最先进的 DS-NER 去噪方法。","text":"[论文阅读]Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning通过自协作去噪学习改进远程监督命名实体识别 摘要远程监督命名实体识别（DS-NER）有效地降低了劳动力成本，但同时由于远程监督的强烈假设，本质上受到标签噪声的影响。通常，错误标记的实例包含大量不完整且不准确的注释噪声，而大多数现有的去噪工作仅涉及一种噪声，无法充分探索整个训练集中的有用信息。为了解决这个问题，我们提出了一种名为自协作去噪学习（SCDL）的强大学习范式，它以互惠互利的方式联合训练两个师生网络，以迭代地执行噪声标签精炼。每个网络都被设计为通过自去噪来利用可靠的标签，并且两个网络相互通信以通过协作去噪来探索不可靠的注释。对五个真实世界数据集的大量实验结果表明 SCDL 优于最先进的 DS-NER 去噪方法。 大纲以下是论文《Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning》的要点： 该论文提出了一种名为Self-Collaborative Denoising Learning（SCDL）的新框架，用于改进远程监督的命名实体识别（DS-NER）。 DS-NER存在标签噪声问题，包括不完整的注释（由于资源覆盖有限）和不准确的注释（由于歧义）。 SCDL包括两个教师-学生网络，分别在自我去噪（内循环）和协同去噪（外循环）方面进行迭代。 对于自我去噪，每个教师-学生网络基于一致性和置信度选择可靠的标签，用于训练学生。通过学生的指数移动平均更新教师。 对于协同去噪，一个教师的伪标签被用来定期更新另一个网络的嘈杂标签。 对5个数据集的实验证明SCDL优于先前DS-NER领域的最新方法。它处理不完整和不准确的注释，而无需假设。 消融实验证实了自协同去噪框架的合理性和稳健性。 SCDL从整个训练集中探索有用信息，而不仅仅是像以前的工作那样减少标签噪声的影响。 总体而言，其主要贡献是提出了一种新的DS-NER训练范式，可以迭代地去噪标签并充分利用错误标记的数据。自协同方法在这种嘈杂标签的情境中被证明是有效的。 SCDL在这项工作中，我们努力弥合这一差距，并提出了一个名为 SCDL（自我协作去噪学习）的强大学习框架。 SCDL 共同训练两个师生网络，形成内环和外环，用于在没有任何假设的情况下应对标签噪声，并对错误标签的数据进行充分探索。每个师生网络内的内循环是一个自去噪方案，用于从两种噪声标签中选择可靠的注释，两个网络之间的外循环是一个协作去噪过程，用于将不可靠的实例纠正为有用的实例。具体来说，在内循环中，每个师生网络选择教师生成的一致且高置信度的标记令牌来训练学生，然后基于重新训练的学生通过指数移动平均（EMA）2逐步更新教师。至于外环，由于 EMA 的稳定性和两个网络之间不同的噪声敏感度，一个网络的老师生成的高质量伪标签被用来更新另一个网络的噪声标签。而且，内循环和外循环过程将交替执行。显然，成功的自降噪过程（内循环）可以生成高质量的伪标签，这对协作学习过程（外循环）有很大帮助，而有前途的外循环将通过细化噪声标签来促进内循环，从而处理标签噪声在 DS-NER 中有效。 师生网络 Student and Teacher network指一个较小且较简单的模型（学生）被训练来模仿一个较大且较复杂的模型（教师）的行为或预测。教师网络通常是一个经过训练在大型数据集上并在特定任务上表现良好的模型。而学生网络被设计成计算效率高且参数较少的模型 Student and Teacher network主要思想是利用教师网络的知识和经验来指导学生网络的学习过程。训练的过程被称为”知识蒸馏”。它涉及使用教师网络的预测结果和真实标签来训练学生网络。学生网络的损失函数通常由两部分组成：标准的交叉熵损失和蒸馏损失。蒸馏损失用来度量学生的预测与教师的软化、更概率化的输出之间的接近程度。 知识蒸馏（Knowledge Distillation）是一种深度学习中的模型优化技术，用于将一个大型、复杂的模型的知识转移给一个小型、简单的模型。它的主要目标是让小型模型能够模仿大型模型的行为，并且在学习过程中，将大型模型的”知识”蒸馏（传递）给小型模型，使得小型模型在表现上能够接近或者甚至超过大型模型。基本思想是通过在训练过程中，使用大型模型的预测结果（通常是类别概率）来指导小型模型的学习过程。一般来说，知识蒸馏会在损失函数中添加一个额外的项，该项用于衡量小型模型的预测与大型模型的预测之间的相似性。这个相似性度量通常使用交叉熵损失函数或其他类似的距离度量。 核心工作在这项工作中，我们努力弥合这一差距，并提出了一个名为 SCDL（自我协作去噪学习）的强大学习框架。 SCDL 共同训练两个师生网络，形成内环和外环，用于在没有任何假设的情况下应对标签噪声，并对错误标签的数据进行充分探索。每个师生网络内的内循环是一个自去噪方案，用于从两种噪声标签中选择可靠的注释，两个网络之间的外循环是一个协作去噪过程，用于将不可靠的实例纠正为有用的实例。具体来说，在内循环中，每个师生网络选择教师生成的一致且高置信度的标记令牌来训练学生，然后基于重新训练的学生通过指数移动平均（EMA）逐步更新教师。至于外环，由于 EMA 的稳定性和两个网络之间不同的噪声敏感度，一个网络的老师生成的高质量伪标签被用来更新另一个网络的噪声标签。而且，内循环和外循环过程将交替执行。显然，成功的自降噪过程（内循环）可以生成高质量的伪标签，这对协作学习过程（外循环）有很大帮助，而有前途的外循环将通过细化噪声标签来促进内循环，从而处理标签噪声在 DS-NER 中有效。 知识蒸馏（Knowledge Distillation）是一种深度学习中的模型优化技术，用于将一个大型、复杂的模型的知识转移给一个小型、简单的模型。它的主要目标是让小型模型能够模仿大型模型的行为，并且在学习过程中，将大型模型的”知识”蒸馏（传递）给小型模型，使得小型模型在表现上能够接近或者甚至超过大型模型。 DS-NER DS-NER是distantly supervised named entity recognition的缩写,表示基于远程监督的命名实体识别。 命名实体识别(NER)任务是识别文本中出现的实体 span,并对其进行分类,如人名、地名组织名等。远程监督是一种半自动标注文本的方法。 DS-NER 的基本思想是: 使用外部资源(知识图谱、词典等)中已知的实体词条,去匹配未标注的文本中出现的词语。 如果一个词语能在外部资源中找到,就自动标注为相应类别的命名实体。 这样可以快速地为大规模文本生成标注,构建用于 NER 模型训练的数据集。 与人工标注相比,DS-NER 大幅减少了标注成本,但也会引入一些噪声: 不完整标注:文本中某些实体无法在外部资源中找到,被错误标注为非实体。 不准确标注:同一词语根据上下文可能属于不同类型,简单匹配会引入歧义。 EMA EMA是Exponential Moving Average的缩写,表示指数移动平均。它是一种给予最近数据更高权重的平均方法,用于模型参数的更新。 具体来说,EMA的计算公式如下: EMA_t &#x3D; α * EMA_{t-1} + (1 - α) * 当前值 这里: EMA_t 是当前时刻t的指数移动平均值 EMA_{t-1} 是上一时刻的指数移动平均值 当前值是新的值 α是平滑系数,取值在0到1之间。 可以观察到,上一时刻的指数移动平均值EMA_{t-1}会与当前新值进行加权平均。α控制老值的权重,近期数据的权重为1-α。 当α取值越大,给予历史值的权重就越大,平滑效果更明显。 EMA的优点是: 提高了模型参数更新的稳定性和平滑性。 相比普通移动平均,EMA给予了最近数据更高的权重。 相比只使用当前信息,EMA考虑了历史信息。 在SCDL中,利用EMA来更新老师模型的参数,可以提供更可靠和稳定的伪标签,从而指导学生模型的训练。这比单次更新参数的效果更好。 根据第四部分，我对该模型的理解是： 自我去噪学习(Self Denoising Learning) 这个部分提出了一个Teacher-Student网络来进行自我去噪。 (1) Teacher模型先基于当前参数,对训练数据生成伪标签。 (2) 然后根据两点选择可靠的标注: 一致性预测:如果伪标签和原始噪声标签一致,则视为可靠。 高置信度预测:如果伪标签的预测概率很高,则可靠。 (3) 把选择的可靠标注和原始标签组合,用于训练Student。 (4) Student通过反向传播更新参数。 (5) 使用指数滑动平均(EMA)来更新Teacher,EMA可以平滑和稳定参数。 通过上面这个内循环,可以在训练过程中去除噪声,提炼可靠标注。 协同去噪学习(Collaborative Denoising Learning) 这个部分提出使用两个Teacher-Student网络,让它们互相协同去噪。 (1) 定期使用一个网络的Teacher生成的伪标签,来更新另一个网络的噪声标签。 (2) 两个网络有不同的学习能力,可以探索对方漏掉的信息,实现协同。 (3) 内循环去除自身噪声,外循环可以纠正对方的错误标注。 (4) 两者交替进行,相互促进,可以不断提炼标注质量。 这样通过协同学习,可以探索原始标签中的更多有用信息,防止过度去噪。 总体流程 (1) 使用原始噪声标签进行预训练 (2) 开始自我去噪和协同去噪的内外循环交替进行 (3) 循环一定步数后,在Teacher和Student中选择最好的模型 通过这个方式,可以充分发挥噪声标注中包含的有用信息,实现名称实体识别任务的去噪。","categories":[{"name":"EMNLP","slug":"EMNLP","permalink":"http://example.com/categories/EMNLP/"}],"tags":[{"name":"DS-NER","slug":"DS-NER","permalink":"http://example.com/tags/DS-NER/"}]},{"title":"YuQi's Blog","slug":"YuQi-s-Blog","date":"2022-09-21T07:44:32.000Z","updated":"2023-12-21T09:55:55.915Z","comments":true,"path":"2022/09/21/YuQi-s-Blog/","permalink":"http://example.com/2022/09/21/YuQi-s-Blog/","excerpt":"论文阅读&#x2F;技术学习&#x2F;知识积累","text":"论文阅读&#x2F;技术学习&#x2F;知识积累","categories":[],"tags":[]}],"categories":[{"name":"arxiv","slug":"arxiv","permalink":"http://example.com/categories/arxiv/"},{"name":"总结","slug":"总结","permalink":"http://example.com/categories/%E6%80%BB%E7%BB%93/"},{"name":"USENIX","slug":"USENIX","permalink":"http://example.com/categories/USENIX/"},{"name":"EMNLP","slug":"EMNLP","permalink":"http://example.com/categories/EMNLP/"}],"tags":[{"name":"综述","slug":"综述","permalink":"http://example.com/tags/%E7%BB%BC%E8%BF%B0/"},{"name":"LLM","slug":"LLM","permalink":"http://example.com/tags/LLM/"},{"name":"NLP","slug":"NLP","permalink":"http://example.com/tags/NLP/"},{"name":"系统安全","slug":"系统安全","permalink":"http://example.com/tags/%E7%B3%BB%E7%BB%9F%E5%AE%89%E5%85%A8/"},{"name":"DS-NER","slug":"DS-NER","permalink":"http://example.com/tags/DS-NER/"}]}